{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e7d29b-9d46-4503-91a7-154895bced57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Akhan\\anaconda3\\envs\\rag_hci\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain-community==0.2.0 sentence-transformers faiss-cpu pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "577e5f41-4ba9-4b34-bcd3-d70cf8a423b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 250\n"
     ]
    }
   ],
   "source": [
    "#load & split\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf_dir = Path(\"../data/raw/hci_papers\")\n",
    "docs = []\n",
    "for pdf in pdf_dir.glob(\"*.pdf\"):\n",
    "    docs.extend(PyPDFLoader(str(pdf)).load())\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=150)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(\"Chunks:\", len(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b2569-832c-4be6-9c07-50abd6f7dd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akhan\\anaconda3\\envs\\rag_hci\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Akhan\\anaconda3\\envs\\rag_hci\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Akhan\\.cache\\huggingface\\hub\\models--intfloat--e5-mistral-7b-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 2 files:   0%|                                                                          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# embed & save FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-mistral-7b-instruct\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "vectordb = FAISS.from_documents(chunks, embedder)\n",
    "vectordb.save_local(\"../data/processed/faiss_hci\")\n",
    "print(\"✔ FAISS index saved to data/processed/faiss_hci\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e08ba9-6629-4a36-9d31-b79ee6868dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea7899-4b44-481b-9d43-9e723ef4dd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290eb51b-f8c2-43a2-80ac-e9adb1721cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94798f3d-4b21-4ed4-a2fc-b75b13fc3ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be506b36-8aaf-4d27-acb1-9d863523f57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca56e0-ad55-4ab1-ba2b-39db3166f841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82cab507-245b-4637-a297-ad18ce68cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain-community==0.2.0 sentence-transformers faiss-cpu pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21782c7-18c7-4ab5-ba45-dc4554c1a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074293de-df40-4595-b5fe-ef1f59f1069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  PDF chunks: 250\n"
     ]
    }
   ],
   "source": [
    "pdf_dir = Path(\"../data/raw/hci_papers\")\n",
    "docs = []\n",
    "for pdf in pdf_dir.glob(\"*.pdf\"):\n",
    "    docs.extend(PyPDFLoader(str(pdf)).load())\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=150)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(\"✅  PDF chunks:\", len(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7f1d9c-3457-4bf7-8a3d-26a5f31fb78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.1.0-cp37-abi3-win_amd64.whl.metadata (498 bytes)\n",
      "Downloading hf_xet-1.1.0-cp37-abi3-win_amd64.whl (4.2 MB)\n",
      "   ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.8/4.2 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.6/4.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 2.4/4.2 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.4/4.2 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.2/4.2 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.2/4.2 MB 3.8 MB/s eta 0:00:00\n",
      "Installing collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e688cf1e-8ffc-4895-9dca-d62a37f40e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akhan\\anaconda3\\envs\\rag_hci\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Akhan\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Sanitising:   0%|                                                                              | 0/250 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Sanitising: 100%|███████████████████████████████████████████████████████████████████| 250/250 [00:00<00:00, 490.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ kept 249 / 250 clean chunks\n",
      "✔  FAISS index saved with 249 entries\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from tqdm import tqdm\n",
    "\n",
    "#EMB_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"   # small & fast\n",
    "#EMB_MODEL = \"intfloat/e5-small-v2\"\n",
    "EMB_MODEL =\"BAAI/bge-small-en-v1.5\"\n",
    "embedder  = HuggingFaceEmbeddings(\n",
    "    model_name=EMB_MODEL,\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "good_texts, good_meta = [], []\n",
    "for txt, meta in tqdm(zip(texts, metadatas), total=len(texts), desc=\"Sanitising\"):\n",
    "    try:\n",
    "        s = str(txt).strip()\n",
    "        if not s:\n",
    "            continue                      # skip blanks\n",
    "        # quick smoke‑test: will the tokenizer accept this?\n",
    "        embedder.client.tokenizer.encode(s, max_length=5)\n",
    "        good_texts.append(s)\n",
    "        good_meta.append(meta)\n",
    "    except Exception:\n",
    "        # log / skip problematic chunk\n",
    "        continue\n",
    "\n",
    "print(f\"✓ kept {len(good_texts)} / {len(texts)} clean chunks\")\n",
    "\n",
    "vectordb = FAISS.from_texts(good_texts, embedder, metadatas=good_meta)\n",
    "vectordb.save_local(\"../data/processed/faiss_hci_small\")\n",
    "print(\"✔  FAISS index saved with\", len(good_texts), \"entries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0447e03-a5c4-4595-8353-f13e8ebe8ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First snippet: We developed our design principles specifcally to provide practi-\n",
      "cal and actionable support to design practitioners. Therefore, we \n",
      "undertook a number of eforts to promote their adoption within \n",
      "our organization. \n",
      "(1) Actionable activities. To bridg …\n",
      "Latency: 59 ms\n"
     ]
    }
   ],
   "source": [
    "# sanity‑check retrieval latency\n",
    "import time\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
    "q = \"How should designers incorporate user feedback when building GenAI features?\"\n",
    "t0 = time.time()\n",
    "docs = retriever.get_relevant_documents(q)\n",
    "print(\"First snippet:\", docs[0].page_content[:250], \"…\")\n",
    "print(\"Latency:\", round((time.time()-t0)*1000), \"ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e213dd4f-d48f-49c6-9f34-dd1e656fd21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importance than the generated artifacts themselves. \n",
      "Weisz et al. \n",
      "that involve trade-ofs between model capabilities and user \n",
      "needs, motivated by work that focuses simultaneously on \n",
      "end-users of systems [55] and on designers as strategic and \n",
      "collaborative end-users of guidelines [82, 87]; and \n",
      "• Sensitize designers to the possible risks of generative AI \n",
      "applications and their potential to cause a variety of harms \n",
      "(inadvertent or intentional), and outline processes that could \n",
      "be used to avo\n"
     ]
    }
   ],
   "source": [
    "#Confirm retrieval works\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":3})\n",
    "print(retriever.get_relevant_documents(\n",
    "      \"How do designers incorporate user feedback when building GenAI features?\")[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4336ba97-e428-40c2-9245-e6dfcbe8d8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add '..\\data\\processed\\faiss_hci_small.dvc' '..\\data\\processed\\.gitignore'\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\u280b Checking graph\n",
      "\n",
      "fatal: pathspec 'notebooks/02_ingest_pdfs.ipynb' did not match any files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   Notebook_01.ipynb\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t../Untitled.ipynb\n",
      "\t../data/processed/.gitignore\n",
      "\t../data/processed/faiss_hci_small.dvc\n",
      "\t../data/raw/.gitignore\n",
      "\t02_ingest_pdfs.ipynb\n",
      "\tUntitled.ipynb\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] Unstaged files detected.\n",
      "[INFO] Stashing unstaged files to C:\\Users\\Akhan\\.cache\\pre-commit\\patch1746519708-18012.\n",
      "black................................................(no files to check)Skipped\n",
      "ruff.................................................(no files to check)Skipped\n",
      "fix end of files.....................................(no files to check)Skipped\n",
      "trim trailing whitespace.............................(no files to check)Skipped\n",
      "[INFO] Restored changes from C:\\Users\\Akhan\\.cache\\pre-commit\\patch1746519708-18012.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 files pushed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "# Cell in 02_ingest_pdfs.ipynb\n",
    "\n",
    "!dvc add ../data/processed/faiss_hci_small\n",
    "!git add ../data/processed/faiss_hci_small.dvc notebooks/02_ingest_pdfs.ipynb\n",
    "!git commit -m \"feat: notebook 02 – FAISS index (cleaned) with MiniLM\"\n",
    "!dvc push\n",
    "!git push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c659e7ac-1ec9-4441-98c6-126b4ad69748",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (2469032613.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mgit add ../data/processed/faiss_hci_small.dvc 02_ingest_pdfs.ipynb\u001b[39m\n                                                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "# Stage the correct files\n",
    "git add ../data/processed/faiss_hci_small.dvc 02_ingest_pdfs.ipynb\n",
    "\n",
    "# Optional: stage .gitignore if needed\n",
    "git add ../data/processed/.gitignore\n",
    "\n",
    "# Commit\n",
    "git commit -m \"feat: notebook 02 – FAISS index (cleaned) with MiniLM\"\n",
    "\n",
    "# Push data to DVC remote\n",
    "dvc push\n",
    "\n",
    "# Push code to GitHub\n",
    "git push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eca27f-6c4c-4931-be6b-38e16415f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_hci)",
   "language": "python",
   "name": "rag_hci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
