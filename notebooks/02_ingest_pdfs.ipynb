{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e7d29b-9d46-4503-91a7-154895bced57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Akhan\\anaconda3\\envs\\rag_hci\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain-community==0.2.0 sentence-transformers faiss-cpu pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "577e5f41-4ba9-4b34-bcd3-d70cf8a423b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 250\n"
     ]
    }
   ],
   "source": [
    "#load & split\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf_dir = Path(\"../data/raw/hci_papers\")\n",
    "docs = []\n",
    "for pdf in pdf_dir.glob(\"*.pdf\"):\n",
    "    docs.extend(PyPDFLoader(str(pdf)).load())\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=150)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(\"Chunks:\", len(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b2569-832c-4be6-9c07-50abd6f7dd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akhan\\anaconda3\\envs\\rag_hci\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Akhan\\anaconda3\\envs\\rag_hci\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Akhan\\.cache\\huggingface\\hub\\models--intfloat--e5-mistral-7b-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 2 files:   0%|                                                                          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# embed & save FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-mistral-7b-instruct\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "vectordb = FAISS.from_documents(chunks, embedder)\n",
    "vectordb.save_local(\"../data/processed/faiss_hci\")\n",
    "print(\"✔ FAISS index saved to data/processed/faiss_hci\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e08ba9-6629-4a36-9d31-b79ee6868dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea7899-4b44-481b-9d43-9e723ef4dd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290eb51b-f8c2-43a2-80ac-e9adb1721cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94798f3d-4b21-4ed4-a2fc-b75b13fc3ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be506b36-8aaf-4d27-acb1-9d863523f57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca56e0-ad55-4ab1-ba2b-39db3166f841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82cab507-245b-4637-a297-ad18ce68cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain-community==0.2.0 sentence-transformers faiss-cpu pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c21782c7-18c7-4ab5-ba45-dc4554c1a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074293de-df40-4595-b5fe-ef1f59f1069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 653 chunks from 8 PDFs\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf_dir = Path(\"../data/raw/hci_papers\")\n",
    "docs = []\n",
    "for pdf in pdf_dir.glob(\"*.pdf\"):\n",
    "    docs.extend(PyPDFLoader(str(pdf)).load())\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=150)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# --- extract plain strings & metadata ---\n",
    "texts      = [doc.page_content for doc in chunks]\n",
    "metadatas  = [doc.metadata     for doc in chunks]\n",
    "\n",
    "print(f\"Loaded {len(texts)} chunks from {len(list(pdf_dir.glob('*.pdf')))} PDFs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f7f1d9c-3457-4bf7-8a3d-26a5f31fb78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in c:\\users\\akhan\\anaconda3\\envs\\rag_hci\\lib\\site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e688cf1e-8ffc-4895-9dca-d62a37f40e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|█████████████████████████████████████████████████████████████████████| 653/653 [01:41<00:00,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ embedded 652 / 653 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     21\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ embedded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(good_texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m vectordb = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgood_vecs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgood_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgood_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m vectordb.save_local(\u001b[33m\"\u001b[39m\u001b[33m../data/processed/faiss_hci_small\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✔ FAISS index saved\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\rag_hci\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1006\u001b[39m, in \u001b[36mFAISS.from_embeddings\u001b[39m\u001b[34m(cls, text_embeddings, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    977\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_embeddings\u001b[39m(\n\u001b[32m    979\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    984\u001b[39m     **kwargs: Any,\n\u001b[32m    985\u001b[39m ) -> FAISS:\n\u001b[32m    986\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m    987\u001b[39m \n\u001b[32m    988\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m \u001b[33;03m            faiss = FAISS.from_embeddings(text_embedding_pairs, embeddings)\u001b[39;00m\n\u001b[32m   1005\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     texts, embeddings = \u001b[38;5;28mzip\u001b[39m(*text_embeddings)\n\u001b[32m   1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1008\u001b[39m         \u001b[38;5;28mlist\u001b[39m(texts),\n\u001b[32m   1009\u001b[39m         \u001b[38;5;28mlist\u001b[39m(embeddings),\n\u001b[32m   (...)\u001b[39m\u001b[32m   1013\u001b[39m         **kwargs,\n\u001b[32m   1014\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "EMB_MODEL = \"BAAI/bge-small-en-v1.5\"               # or MiniLM / e5-small\n",
    "embedder  = HuggingFaceEmbeddings(\n",
    "    model_name=EMB_MODEL,\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "good_vecs, good_texts, good_meta = [], [], []\n",
    "\n",
    "for txt, meta in tqdm(zip(texts, metadatas), total=len(texts), desc=\"Embedding\"):\n",
    "    try:\n",
    "        vec = embedder.embed_query(str(txt))       # encodes ONE chunk\n",
    "        good_vecs.append(vec)\n",
    "        good_texts.append(str(txt))\n",
    "        good_meta.append(meta)\n",
    "    except Exception as e:\n",
    "        # If one chunk is malformed, skip & continue\n",
    "        continue\n",
    "\n",
    "print(f\"✓ embedded {len(good_texts)} / {len(texts)} chunks\")\n",
    "\n",
    "vectordb = FAISS.from_embeddings(good_vecs, good_texts, good_meta)\n",
    "vectordb.save_local(\"../data/processed/faiss_hci_small\")\n",
    "print(\"✔ FAISS index saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0447e03-a5c4-4595-8353-f13e8ebe8ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity‑check retrieval latency\n",
    "import time\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
    "q = \"How should designers incorporate user feedback when building GenAI features?\"\n",
    "t0 = time.time()\n",
    "docs = retriever.get_relevant_documents(q)\n",
    "print(\"First snippet:\", docs[0].page_content[:250], \"…\")\n",
    "print(\"Latency:\", round((time.time()-t0)*1000), \"ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e213dd4f-d48f-49c6-9f34-dd1e656fd21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importance than the generated artifacts themselves. \n",
      "Weisz et al. \n",
      "that involve trade-ofs between model capabilities and user \n",
      "needs, motivated by work that focuses simultaneously on \n",
      "end-users of systems [55] and on designers as strategic and \n",
      "collaborative end-users of guidelines [82, 87]; and \n",
      "• Sensitize designers to the possible risks of generative AI \n",
      "applications and their potential to cause a variety of harms \n",
      "(inadvertent or intentional), and outline processes that could \n",
      "be used to avo\n"
     ]
    }
   ],
   "source": [
    "#Confirm retrieval works\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":3})\n",
    "print(retriever.get_relevant_documents(\n",
    "      \"How do designers incorporate user feedback when building GenAI features?\")[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4336ba97-e428-40c2-9245-e6dfcbe8d8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add '..\\data\\processed\\faiss_hci_small.dvc' '..\\data\\processed\\.gitignore'\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\u280b Checking graph\n",
      "\n",
      "fatal: pathspec 'notebooks/02_ingest_pdfs.ipynb' did not match any files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   Notebook_01.ipynb\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t../Untitled.ipynb\n",
      "\t../data/processed/.gitignore\n",
      "\t../data/processed/faiss_hci_small.dvc\n",
      "\t../data/raw/.gitignore\n",
      "\t02_ingest_pdfs.ipynb\n",
      "\tUntitled.ipynb\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] Unstaged files detected.\n",
      "[INFO] Stashing unstaged files to C:\\Users\\Akhan\\.cache\\pre-commit\\patch1746519708-18012.\n",
      "black................................................(no files to check)Skipped\n",
      "ruff.................................................(no files to check)Skipped\n",
      "fix end of files.....................................(no files to check)Skipped\n",
      "trim trailing whitespace.............................(no files to check)Skipped\n",
      "[INFO] Restored changes from C:\\Users\\Akhan\\.cache\\pre-commit\\patch1746519708-18012.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 files pushed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "# Cell in 02_ingest_pdfs.ipynb\n",
    "\n",
    "!dvc add ../data/processed/faiss_hci_small\n",
    "!git add ../data/processed/faiss_hci_small.dvc notebooks/02_ingest_pdfs.ipynb\n",
    "!git commit -m \"feat: notebook 02 – FAISS index (cleaned) with MiniLM\"\n",
    "!dvc push\n",
    "!git push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c659e7ac-1ec9-4441-98c6-126b4ad69748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'notebooks/02_ingest_pdfs.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    }
   ],
   "source": [
    "!git add ../data/processed/faiss_hci_small.dvc 02_ingest_pdfs.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84eca27f-6c4c-4931-be6b-38e16415f52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 8d3ac95] feat: notebook 02 â€“ FAISS index (cleaned) with MiniLM\n",
      " 2 files changed, 492 insertions(+)\n",
      " create mode 100644 data/processed/faiss_hci_small.dvc\n",
      " create mode 100644 notebooks/02_ingest_pdfs.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] Unstaged files detected.\n",
      "[INFO] Stashing unstaged files to C:\\Users\\Akhan\\.cache\\pre-commit\\patch1746520025-13576.\n",
      "black................................................(no files to check)Skipped\n",
      "ruff.................................................(no files to check)Skipped\n",
      "fix end of files.........................................................Passed\n",
      "trim trailing whitespace.................................................Passed\n",
      "[INFO] Restored changes from C:\\Users\\Akhan\\.cache\\pre-commit\\patch1746520025-13576.\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"feat: notebook 02 – FAISS index (cleaned) with MiniLM\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71c4c66b-226f-401b-a7c4-8c61a3ffda1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is up to date.\n"
     ]
    }
   ],
   "source": [
    "!dvc push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb1380b8-57fe-4180-a8ca-3659e50534fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To github.com:Asadunipr/rag-hci-interaction.git\n",
      "   60ebf12..8d3ac95  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0cf2ec-1d27-4a1a-8a15-9bbc5bed594d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectordb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvectordb\u001b[49m.save_local(\u001b[33m\"\u001b[39m\u001b[33m../data/processed/faiss_hci_small\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'vectordb' is not defined"
     ]
    }
   ],
   "source": [
    "vectordb.save_local(\"../data/processed/faiss_hci_small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988af0e-c91e-4a84-bf6b-15e7298e44f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_hci)",
   "language": "python",
   "name": "rag_hci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
